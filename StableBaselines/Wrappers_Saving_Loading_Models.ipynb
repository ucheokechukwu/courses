{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d9ec7f-1ac6-4a5b-9a97-17768db6c5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for autoformatting\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0d916b-cc56-4d0a-ad2b-a6076b5c7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6feacada-c39c-48f1-a760-fbf512ffbd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre saved (array([0.3014052], dtype=float32), None)\n",
      "loaded (array([0.3014052], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=0).learn(8_000)\n",
    "# The model will be saved under PPO_tutorial.zip\n",
    "model.save(f\"{save_dir}/PPO_tutorial\")\n",
    "\n",
    "# sample an observation from the environment\n",
    "obs = model.env.observation_space.sample()\n",
    "\n",
    "# Check prediction before saving\n",
    "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
    "\n",
    "# del model  # delete trained model to demonstrate loading\n",
    "\n",
    "loaded_model = PPO.load(f\"{save_dir}/PPO_tutorial\")\n",
    "# Check that the prediction is the same after loading (for the same observation)\n",
    "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dada048-f828-403b-a6e8-ab8729662485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: gamma=0.9, n_steps=20\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1582      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -0.000236 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -62.3     |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.29e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1539      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -0.000144 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -35.8     |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 896       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1580      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -0.000122 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -46.1     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 980       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1568      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -8.25e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -44.1     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 708       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x11cf3f620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loading function can also update the model's class variables when loading.\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"Pendulum-v1\", verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "# The model will be saved under A2C_tutorial.zip\n",
    "model.save(f\"{save_dir}/A2C_tutorial\")\n",
    "\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# load the model, and when loading set verbose to 1\n",
    "loaded_model = A2C.load(f\"{save_dir}/A2C_tutorial\", verbose=1)\n",
    "\n",
    "# show the save hyperparameters\n",
    "print(f\"loaded: gamma={loaded_model.gamma}, n_steps={loaded_model.n_steps}\")\n",
    "\n",
    "# as the environment is not serializable, we need to set a new instance of the environment\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")]))\n",
    "# and continue training\n",
    "loaded_model.learn(8_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1c25f-2cba-45a9-a5c9-7f10bfded2d8",
   "metadata": {},
   "source": [
    "### Gym and VecEnv wrappers\n",
    "\n",
    "https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "\n",
    "https://gymnasium.farama.org/api/wrappers/\n",
    "\n",
    "**Anatomy of a gym wrapper**: A gym wrapper follows the gym interface.  It has a `reset()` and `step()` method. We can access the environment with `self.env`, without modifying the original env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4bd1ec7-dcee-4806-9465-35e25f1be6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "    \"\"\":param env : (gym.Env) Gym environment that will be wrapped\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"reset the environment\"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(step, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float or int) action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\n",
    "        is the max number ofsteps reached (episode finished artificially), additional informations\n",
    "        \"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515944c4-543f-4800-8cb6-fb14ebe90b18",
   "metadata": {},
   "source": [
    "#### 1. Example - `TimeLimitWrapper` limit the episode length\n",
    "* In practice, gym already have a wrapper for that named `TimeLimit (gym.wrappers.TimeLimit)` that is used by most environments.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c58841ee-c2b6-4b7b-a64c-a48fa70a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to limit the number of steps by episode,\n",
    "# need to overwrite the done signal when the limit is reached.\n",
    "# and pass that information in the info dictionary.\n",
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, max_steps=100):\n",
    "        # call the parent constructor, so we can access self.env later\n",
    "        super().__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        self.current_steps = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"reset the environment\"\"\"\n",
    "        self.current_step = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float or int) action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\n",
    "        is the max number ofsteps reached (episode finished artificially), additional informations\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # Overwrite the truncation signal when when the number of steps reaches the maximum\n",
    "        if self.current_step >= self.max_steps:\n",
    "            truncated = True\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b5fcf12-b67d-476a-9c23-4399bbc1919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without the TimeWrapper\n",
      "5000 {}\n",
      "\n",
      "With the TimeWrapper\n",
      "100 {}\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "from gymnasium.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wrap the environment in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done and n_steps < 5000:\n",
    "    # Take random actions\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    done = terminated or truncated\n",
    "    n_steps += 1\n",
    "print(\"Without the TimeWrapper\")\n",
    "print(n_steps, info)\n",
    "\n",
    "\n",
    "# Wrap the environment\n",
    "env = TimeLimitWrapper(env, max_steps=100)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "    # Take random actions\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    done = terminated or truncated\n",
    "    n_steps += 1\n",
    "print(\"\\nWith the TimeWrapper\")\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ea482-f19a-4462-b5ec-eeaafad3c35f",
   "metadata": {},
   "source": [
    "#### 2. Example - `NormalizeActionWrapper` normalize observations and actions before giving it to the agent.\n",
    "* In practice, gym already have a wrapper for that named `TimeLimit (gym.wrappers.TimeLimit)` that is used by most environments.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4061de2a-3920-4664-adbd-2d06f369b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # retrieve the action space\n",
    "        action_space = env.action_space\n",
    "        assert isinstance(action_space, gym.spaces.Box)  # this is for continuous action\n",
    "        # retrieve max/min values\n",
    "        self.low, self.high = action_space.low, action_space.high\n",
    "        # modify the action space to [-1, 1]\n",
    "        env.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n",
    "        )\n",
    "        # call the parent constructor\n",
    "        super().__init__(env)\n",
    "\n",
    "    def rescale_action(self, scaled_action):\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "        :param scaled_action: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n",
    "        \"\"\"\n",
    "        # Rescale action from [-1, 1] to original [low, high] interval\n",
    "        rescaled_action = self.rescale_action(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ef0e635-c830-4973-bb18-31cd58261ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before rescaling\n",
      "[-2.]\n",
      "[-0.80088085]\n",
      "[-0.8478717]\n",
      "[0.37158832]\n",
      "[0.6830635]\n",
      "[0.26569128]\n",
      "[0.33604035]\n",
      "[-0.9614514]\n",
      "[-1.5475844]\n",
      "[-0.4128491]\n",
      "[0.17539002]\n",
      "\n",
      "\n",
      "after rescaling\n",
      "[-1.]\n",
      "[0.70716596]\n",
      "[-0.7957447]\n",
      "[0.33592948]\n",
      "[-0.5850199]\n",
      "[-0.16451477]\n",
      "[-0.40088794]\n",
      "[-0.6694457]\n",
      "[-0.39624718]\n",
      "[-0.09851905]\n",
      "[-0.38071966]\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "# before rescaling\n",
    "\n",
    "original_env = gym.make(\"Pendulum-v1\")\n",
    "print(\"before rescaling\")\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())\n",
    "\n",
    "# after rescaling\n",
    "print(\"\\n\\nafter rescaling\")\n",
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b83b98-08c5-466b-b168-d64629dfc251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before rescaling\n",
      "mean_reward: -1506.35 +/- 240.18\n",
      "\n",
      "\n",
      "after rescaling\n",
      "mean_reward: -1652.65 +/- 182.49\n"
     ]
    }
   ],
   "source": [
    "## testing with a RL algorithm\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = Monitor(gym.make(\"Pendulum-v1\"))\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0).learn(int(10000))\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, env, n_eval_episodes=100, warn=False, deterministic=True\n",
    ")\n",
    "print(\"before rescaling\")\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "\n",
    "# with the action wrapper\n",
    "normalized_env = Monitor(gym.make(\"Pendulum-v1\"))\n",
    "# Note that we can use multiple wrappers\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])\n",
    "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=0).learn(int(10000))\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model_2, env, n_eval_episodes=100, warn=False, deterministic=True\n",
    ")\n",
    "print(\"\\n\\nafter rescaling\")\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0d618-c3e6-4808-97d3-edaad9074aab",
   "metadata": {},
   "source": [
    "### Monitor Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea403534-72f6-4ffd-a038-52ccc8ab277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to monitor the training progress,\n",
    "# storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n",
    "# return those values using the info dict after each end of episode.\n",
    "\n",
    "\n",
    "class MyMonitorWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, **kwargs):\n",
    "        # call the parent constructor, so we can access self.env later\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = 0\n",
    "        super().__init__(env, **kwargs)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"reset the environment\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float or int) action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\n",
    "        is the max number ofsteps reached (episode finished artificially), additional informations\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        print(type(info))\n",
    "        self.episode_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        print(self.episode_rewards, self.episode_steps)\n",
    "        # if done\n",
    "        if done:\n",
    "            sum_of_episode_rewards = sum(self.episode_rewards)\n",
    "            number_of_episode_steps = self.episode_steps\n",
    "            info[\"sum_of_episode_rewards\"], info[\"number_of_episode_steps\"] = (\n",
    "                sum_of_episode_rewards,\n",
    "                number_of_episode_steps,\n",
    "            )\n",
    "            print(self.episode_rewards, self.episode_steps)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0535570b-6653-402b-ac68-54886086b373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6963424 ,  0.7177097 , -0.64042455]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0).learn(int(1000))\n",
    "\n",
    "# === YOUR CODE HERE ===#\n",
    "# Wrap the environment\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "env = MyMonitorWrapper(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# Reset the environment\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d274275-d628-4881-8fe1-cac31dd150dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The _info=[{'episode': {'r': 21.0, 'l': 21, 't': 1.53533}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.09843981, -0.1932801 , -0.22030337, -0.45737815], dtype=float32)}]\n",
      " at the end of episode=0\n",
      "\n",
      "The _info=[{'episode': {'r': 20.0, 'l': 20, 't': 1.544396}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.05870731,  0.01871654, -0.21634758, -0.6021475 ], dtype=float32)}]\n",
      " at the end of episode=1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseAlgorithm, num_episodes: int = 100, deterministic: bool = True\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `num_episodes`.\n",
    "    :param model: the RL Agent\n",
    "    :param_env: the gym Environment\n",
    "    :param num_episodes: the number of episodes to evaluate it\n",
    "    :param deterministic: whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward for the last `num_episodes`\n",
    "    \"\"\"\n",
    "    # this works for a single environment\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    all_episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            # _states are only useful if using LSTM policites\n",
    "            # `deterministic` is to use deterministic actions\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # actions, rewards, dones are arrays\n",
    "            # because we are using a vectorized env\n",
    "            obs, reward, done, _info = vec_env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "        print(f\"The {_info=}\\n at the end of {episode=}\\n\")\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "\n",
    "evaluate(model, num_episodes=2, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c73a52d-243c-4fe8-9ed4-e9a504f910e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (1, 3) for Box environment, please use (4,) or (n_env, 4) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m action, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:365\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     )\n\u001b[0;32m--> 365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:272\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    268\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/utils.py:401\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_vec_obs_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/utils.py:268\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Unexpected observation shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox environment, please use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor (n_env, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) for the observation shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, observation_space\u001b[38;5;241m.\u001b[39mshape)))\n\u001b[1;32m    272\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (1, 3) for Box environment, please use (4,) or (n_env, 4) for the observation shape."
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "action, *_ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5db048d8-743e-4543-8ae7-f8eaa0952b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[72], line 32\u001b[0m, in \u001b[0;36mMyMonitorWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m:param action: ([float or int) action taken by the agent\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m:return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mis the max number ofsteps reached (episode finished artificially), additional informations\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 32\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(info))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "env.step(action)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5145108e-567c-45d3-b142-7d3e9ac954af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x[\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/RL/rl_venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[72], line 32\u001b[0m, in \u001b[0;36mMyMonitorWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m:param action: ([float or int) action taken by the agent\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m:return: (np.ndarray, float, bool, bool, dict) observation, reward, is this a final state (episode finished),\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mis the max number ofsteps reached (episode finished artificially), additional informations\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 32\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(info))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "x = env.step(action)\n",
    "x[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d6e65-f849-4b7f-a8f3-65a7a76fe9bb",
   "metadata": {},
   "source": [
    "#### Going further - Saving format \n",
    "\n",
    "The format for saving and loading models is a zip-archived JSON dump and NumPy zip archive of the arrays:\n",
    "```\n",
    "saved_model.zip/\n",
    "├── data              JSON file of class-parameters (dictionary)\n",
    "├── parameter_list    JSON file of model parameters and their ordering (list)\n",
    "├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n",
    "    ├── ...           Being a zip-archive itself, this object can also be opened ...\n",
    "        ├── ...       as a zip-archive and browsed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53062d12-1c2b-4ad4-867a-3e90bf8ca61a",
   "metadata": {},
   "source": [
    "#### Save and find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e1e0f-d4a5-4771-a606-3ccf58fcdcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
